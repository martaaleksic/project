I follow a specific process when working on a new **data analysis** project.

Projects are an excellent opportunity to practice the skills you have acquired.

We can apply our newly acquired knowledge to some real-world tasks by developing our own projects.

First, I try to understand the scope of the whole project to learn what it requires.

**Data preparation** is of great importance.

I'll start by importing the following data analysis and visualization libraries:

- NumPy
- Pandas
- Matplotlib
- Seaborn and others...

It's always a good idea to check your new dataset with the info() and describe() functions.

Then, I analyze the missing values (MVA).

These datasets may be missing data points due to failures in the data collection pipeline or fusing data at different granularities.

**Data cleaning** is the process of identifying and removing the anomalies in the dataset.

**Data Transformation** – It involves changing the data type of the columns, creating derived columns, or removing duplicate data, to name a few

That way, I create a DataFrame and prepare the raw data for analysis. After that, I proceed with some exploratory data analysis.

**Exploring and Visualizing the Data –** Perform analysis of the datasets to find hidden insights and patterns in them.

Depending on the data type, I draw multiple charts of different styles to present your data in an intuitive and understandable format that is best for interpretation.

I then analyze the metrics' details, implement my knowledge of the project, and indicate specific relationships that can be useful for further analysis or interpretation.

**Data engineering** is the process of making transformations to and cleansing data. **

It also involves profiling and aggregating data. In other words, data engineering is all about data collection and transforming raw data gathered from several sources into information ready to be used in decision-making.

## In this projects:

1. Using requests to download a file

2. Importing Libraries

3. Loading Dataset 

4. Correlation Strength of the relationship between two variables

5. Missing Values Distribution

6. Calculate the % of missing data from each column

7. Grouping data and finding correlation

8. Selecting columns

9. Logic, Control Flow and Filtering

10. Quantitative comparisons and statistical visualizations

11. Data Manipulation with Python

12. Statistical exploratory data analysis

13. Visualization, customization data along with its libraries, graphs, charts, histogram and more..

# Python libraries

### Matplotlib

Matplotlib is a cross-platform, data visualization and graphical plotting library for **Python** and its numerical extension NumPy.

### Pandas 

Pandas is an open source data analysis library built on top of the **Python** programming language. The import pandas portion of the code tells **Python** to bring the pandas data analysis library into your current environment.

### NupPy

It adds powerful data structures to **Python** that guarantee efficient calculations with arrays and matrices and it supplies an enormous library of high-level mathematical functions that operate on these arrays and matrices.

### Seaborn

Seaborn is an amazing data visualization library for statistical graphics plotting in **Python**. It provides beautiful default styles and colour palettes to make statistical plots more attractive.

### Termcolor

To change the text color, we use a **Python** library called termcolor.

### Plotly

Plotly's **Python** graphing library makes interactive, publication-quality graphs. Examples of how to make line plots, scatter plots, area charts, bar charts...

### Requests

The requests module allows you to send HTTP requests using **Python**.

The HTTP request returns a Response Object with all the response data (content, encoding, status, etc)..