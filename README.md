I follow a specific process when working on a new **data analysis** project.

Projects are an excellent opportunity to practice the skills you have acquired.

We can apply our newly acquired knowledge to some real-world tasks by developing our own projects.

First, I try to understand the scope of the whole project to learn what it requires.

**Data preparation** is of great importance.

I'll start by importing the following data analysis and visualization libraries:

- NumPy
- Pandas
- Matplotlib
- Seaborn and others...

It's always a good idea to check your new dataset with the info() and describe() functions.

Then, I analyze the missing values (MVA).

These datasets may be missing data points due to failures in the data collection pipeline or fusing data at different granularities.

**Data cleaning** is the process of identifying and removing the anomalies in the dataset.

**Data Transformation** – It involves changing the data type of the columns, creating derived columns, or removing duplicate data, to name a few

That way, I create a DataFrame and prepare the raw data for analysis. After that, I proceed with some exploratory data analysis.

**Exploring and Visualizing the Data –** Perform analysis of the datasets to find hidden insights and patterns in them.

Depending on the data type, I draw multiple charts of different styles to present your data in an intuitive and understandable format that is best for interpretation.

I then analyze the metrics' details, implement my knowledge of the project, and indicate specific relationships that can be useful for further analysis or interpretation.

**Data engineering** is the process of making transformations to and cleansing data. **

It also involves profiling and aggregating data. In other words, data engineering is all about data collection and transforming raw data gathered from several sources into information ready to be used in decision-making.

## **In this projects:**

1. Using requests to download a file
2. Importing Libraries
3. Loading Dataset
4. Correlation Strength of the relationship between two variables
5. Missing Values Distribution
6. Calculate the % of missing data from each column
7. Grouping data and finding correlation
8. Selecting columns
9. Logic, Control Flow and Filtering
10. Quantitative comparisons and statistical visualizations
11. Data Manipulation with Python
12. Statistical exploratory data analysis
13. Visualization, customization data along with its libraries, graphs, charts, histogram and more..