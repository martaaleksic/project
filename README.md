I follow a specific process when working on a new **data analysis** project.

Projects are an excellent opportunity to practice the skills you have acquired.

We can apply our newly acquired knowledge to some real-world tasks by developing our own projects.

First, I try to understand the scope of the whole project to learn what it requires.

**Data preparation** is of great importance.

I'll start by importing the following data analysis and visualization libraries:

- NumPy
- Pandas
- Matplotlib
- Seaborn and others...

It's always a good idea to check your new dataset with the info() and describe() functions.

Then, I analyze the missing values (MVA).

These datasets may be missing data points due to failures in the data collection pipeline or fusing data at different granularities.

**Data cleaning** is the process of identifying and removing the anomalies in the dataset.

**Data Transformation** – It involves changing the data type of the columns, creating derived columns, or removing duplicate data, to name a few

That way, I create a DataFrame and prepare the raw data for analysis. After that, I proceed with some exploratory data analysis.

**Exploring and Visualizing the Data –** Perform analysis of the datasets to find hidden insights and patterns in them.

Depending on the data type, I draw multiple charts of different styles to present your data in an intuitive and understandable format that is best for interpretation.

I then analyze the metrics' details, implement my knowledge of the project, and indicate specific relationships that can be useful for further analysis or interpretation.

**Data engineering** is the process of making transformations to and cleansing data. **

It also involves profiling and aggregating data. In other words, data engineering is all about data collection and transforming raw data gathered from several sources into information ready to be used in decision-making.

## **In this projects:**

1. Using requests to download a file
2. Importing Libraries
3. Loading Dataset
4. Correlation Strength of the relationship between two variables
5. Missing Values Distribution
6. Calculate the % of missing data from each column
7. Grouping data and finding correlation
8. Selecting columns
9. Logic, Control Flow and Filtering
10. Quantitative comparisons and statistical visualizations
11. Data Manipulation with Python
12. Statistical exploratory data analysis
13. Visualization, customization data along with its libraries, graphs, charts, histogram and more..

## Python libraries

### Matplotlib
Matplotlib is a cross-platform, data visualization, and graphical plotting library for Python and its numerical extension NumPy.

### Pandas
Pandas is an open-source data analysis library built on top of the Python programming language. The import pandas portion of the code tells Python to bring the pandas data analysis library into your current environment.

### NupPy
It adds powerful data structures to Python that guarantee efficient calculations with arrays and matrices and it supplies an enormous library of high-level mathematical functions that operate on these arrays and matrices.

### Seaborn
Seaborn is an amazing data visualization library for statistical graphics plotting in Python. It provides beautiful default styles and color palettes to make statistical plots more attractive.

### Termcolor
To change the text color, we use a Python library called termcolor.

### Plotly
Plotly's Python graphing library makes interactive, publication-quality graphs. Examples of how to make line plots, scatter plots, area charts, bar charts...

### Requests
The requests module allows you to send HTTP requests using Python.
The HTTP request returns a Response Object with all the response data (content, encoding, status, etc)...